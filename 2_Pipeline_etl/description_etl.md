# üîå Description du pipeline ETL

Ce document fournit une vue d'ensemble du pipeline **ETL (Extract, Transform, Load)** mis en place dans le cadre du projet "Infrastructure Data sur le Cloud".

---

## Objectif du pipeline
Permettre la collecte, le nettoyage, la standardisation et le chargement de donn√©es d‚Äôoffres d‚Äôemploi dans une base PostgreSQL RDS afin de faciliter l'analyse et la visualisation interactive via un dashboard Streamlit.

---

## ETAPE 1 : EXTRACT ‚Äì Collecte de donn√©es

### Sources utilis√©es
- **AI Jobs** (scraping HTML avec `requests` + `BeautifulSoup`)
- **Adzuna API** (requ√™tes `GET` via `requests`)
- **API P√¥le Emploi** (via `requests` avec authentification OAuth2)

### Format de sortie
- Fichiers CSV dans `data/raw/`
  - `aijobs_jobs.csv`
  - `adzuna_jobs.csv`
  - `poleemploi_jobs.csv`

### Scripts concern√©s
```bash
etl/
‚îú‚îÄ‚îÄ scrape_aijobs.py
‚îú‚îÄ‚îÄ fetch_api_adzuna.py
‚îî‚îÄ‚îÄ fetch_api_poleemploi.py
```

---

## ETAPE 2 : TRANSFORM ‚Äì Nettoyage et standardisation

### Traitements appliqu√©s
- Normalisation des noms et des valeurs (source, entreprise, ville)
- Suppression des doublons et valeurs manquantes critiques
- Standardisation des champs `salary_min`, `salary_max`, `contract_type`
- Enrichissement √©ventuel via g√©ocodage ou mapping
- Fusion des datasets via `pandas.concat`

### Objectif
Avoir un fichier unique et propre : `jobs_combined.csv` regroupant toutes les offres standardis√©es

### Script concern√©
```bash
etl/transform_jobs.py
```

### Format de sortie
- `data/cleaned/jobs_combined.csv`

---

## ETAPE 3 : LOAD ‚Äì Chargement en base PostgreSQL RDS

### Outils
- `sqlalchemy` + `psycopg2`
- Connexion √† une instance RDS PostgreSQL sur AWS

### Contenu charg√©
- Table `jobs` dans la base, avec les colonnes :
  - `title`, `company`, `location`, `salary_min`, `salary_max`, `contract_type`, `source`, `publication_date`, etc.

### Script utilis√©
```bash
etl/load_to_rds.py
```

---


## Visualisation interactive

- Dashboard d√©velopp√© avec **Streamlit**
- Visualisations dynamiques avec **Plotly**, **Seaborn**, **Matplotlib**
- Filtrage interactif (localisation, source, entreprise, salaire...)
- Calculs et KPIs int√©gr√©s : insights sur les salaires, les offres, les postes, etc.

---

## Avantages du pipeline
- Modulaire, √©volutif, et maintenable facilement
- Compatible Cloud (RDS, S3)
- Donn√©es nettoy√©es, enrichies et pr√™tes pour la BI
- Permet l'exploration imm√©diate via le dashboard

> Tous les scripts ETL sont pr√©sents dans le dossier `etl/` et comment√©s pour la r√©utilisation.
