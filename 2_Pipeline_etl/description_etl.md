# üîå Description du pipeline ETL

Ce document fournit une vue d'ensemble du pipeline **ETL (Extract, Transform, Load)** mis en place dans le cadre du projet "Infrastructure Data sur le Cloud".

---

## Objectif du pipeline
Permettre la collecte, le nettoyage, la standardisation et le chargement de donn√©es d‚Äôoffres d‚Äôemploi dans une base PostgreSQL RDS afin de faciliter l'analyse et la visualisation.

---

## ETAPE 1 : EXTRACT ‚Äì Collecte de donn√©es

### Sources utilis√©es
- **AI Jobs** (scraping HTML avec `requests` + `BeautifulSoup`)
- **Adzuna API** (requ√™tes `GET` via `requests`)

### Format de sortie
- Fichiers CSV dans `data/raw/`
  - `aijobs_jobs.csv`
  - `adzuna_jobs.csv`

### Script concern√©s
```bash
etl/
‚îú‚îÄ‚îÄ scrape_aijobs.py
‚îî‚îÄ‚îÄ fetch_api_adzuna.py
```

---

## ETAPE 2 : TRANSFORM ‚Äì Nettoyage et standardisation

### Traitements appliqu√©s
- Normalisation des noms de colonnes : `title`, `company`, `location`, `source`
- Suppression des valeurs manquantes
- Fusion des datasets avec `pandas.concat`

### Objectif
Avoir un fichier unique et propre : `jobs_combined.csv`
Ce fichier va regrouper l'ensemble des offres que l'on a extrait depuis Ai Jobs et Adzuna.

### Script concern√©
```bash
etl/transform_jobs.py
```

### Format de sortie
- `data/cleaned/jobs_combined.csv`

---

## ETAPE 3 : LOAD ‚Äì Chargement en base PostgreSQL RDS

### Outils
- `sqlalchemy` + `psycopg2`
- Connexion √† une instance RDS PostgreSQL

### Contenu charg√©
- Fichier `jobs_combined.csv` ins√©r√© dans une table `job_offers`

### Script utilis√©
```bash
etl/load_to_rds.py
```

---

## Automatisation
L'ensemble du pipeline peut √™tre r√©ex√©cut√© en local ou en cloud avec les scripts Python.


---

## Avantages du pipeline
- Facilement maintenable et r√©utilisable
- Compatible cloud (S3, RDS)
- Donn√©es standardis√©es, pr√™tes pour l'analyse

> Tous les scripts ETL sont pr√©sents dans le dossier `etl/` et document√©s en commentaires.
